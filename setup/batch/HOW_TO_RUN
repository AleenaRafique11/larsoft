export GROUP=lbne
. /grid/fermiapp/common/tools/setup_condor.sh
# to run 5 copies on grid. Drop -g for local-cluster. The -X509 is to pick up the right proxy. -opportunistic picks up extra nodes from other experiments if they're available. You want this!
# file on gpsn01 if you have multiple kproxys going on.

jobsub -X509_USER_PROXY  /scratch/USER/grid/USER.lbne.proxy -g -N 5 -opportunistic -dOUT /lbne/data/users/USER/mu800 -q condor_lBdetMC.sh `whoami`  `pwd`
 
## Add extra jobsub flag for the special 4GB (not 2!!) gpcf (local) nodes.
##
##   -c '((TARGET.Machine =?= "gpwn007.fnal.gov") || (TARGET.Machine =?= "gpwn006.fnal.gov"))'
##

# Then, condor_q USER    # to see job status.
# condor_userprio -all   # to see where you stand on the food chain. 
# 			 # A (High)Low number is (un)favorable.
# condor_status          # to see how many cores are available.


##### Remember: with the -g option the following has to be done in advance.
#> Define the $HOME variable in condor_lBdetMC.sh to your condor job launch
#> directory. (Not, e.g., your afs home space) and give /EXPERIMENT/data/users/USER and
#> /grid/data/EXPERIMENT/outstage/USER 775 permissions. 
#> cd /lbne/data/users; chmod -R 775 echurch)

# The -dOUT /lbne/data/users/echurch/ is some under-the-hood mojo
# that allows the grid/local nodes to write our output root files to CONDOR_DIR_OUT 
# living on the node itself (note the explicit wirting of such output to that dir in the 
# condor_lBdetMC.sh file), and then cpn over to the /lbne/data/users bluearc disks
# when the job has completed. 
# (cpn knows how to throttle the cp process so as not to clobber the bluearc disks).